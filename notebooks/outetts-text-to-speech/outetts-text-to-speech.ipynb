{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Speech synthesis using OuteTTS and OpenVINO\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Important note:</b> This notebook requires python >= 3.10. Please make sure that your environment fulfill to this requirement before running it </div>\n",
    "\n",
    "[OuteTTS-0.1-350M](https://huggingface.co/OuteAI/OuteTTS-0.1-350M) is a novel text-to-speech synthesis model that leverages pure language modeling without external adapters or complex architectures, built upon the LLaMa architecture. It demonstrates that high-quality speech synthesis is achievable through a straightforward approach using crafted prompts and audio tokens.\n",
    "\n",
    "More details about model can be found in [original repo](https://github.com/edwko/OuteTTS).\n",
    "\n",
    "In this tutorial we consider how to run OuteTTS pipeline using OpenVINO.\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Convert model](#Convert-model)\n",
    "- [Run model inference](#Run-model-inference)\n",
    "    - [Text-to-Speech generation](#Text-to-Speech-generation)\n",
    "    - [Text-to-Speech generation with Voice Cloning](#Text-to-Speech-generation-with-Voice-Cloning)\n",
    "- [Quantization](#Quantization)\n",
    "    - [Prepare calibration dataset](#Prepare-calibration-dataset)\n",
    "    - [Quantize model](#Quantize-model)\n",
    "    - [Verifying quantized model execution](#Verifying-quantized-model-execution)\n",
    "    - [Comparing original and quantized model performance](#Comparing-model-performance)\n",
    "- [Interactive demo](#Interactive-demo)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/outetts-text-to-speech/outetts-text-to-speech.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "utility_files = [\"skip_kernel_extension.py\", \"cmd_helper.py\", \"notebook_utils.py\", \"pip_helper.py\"]\n",
    "base_utility_url = \"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/\"\n",
    "\n",
    "for utility_file in utility_files:\n",
    "    if not Path(utility_file).exists():\n",
    "        r = requests.get(base_utility_url + utility_file)\n",
    "        with Path(utility_file).open(\"w\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "\n",
    "helper_files = [\"gradio_helper.py\", \"ov_outetts_helper.py\"]\n",
    "base_helper_url = \"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/outetts-text-to-speech\"\n",
    "\n",
    "for helper_file in helper_files:\n",
    "    if not Path(helper_file).exists():\n",
    "        r = requests.get(base_helper_url + helper_file)\n",
    "        with Path(helper_file).open(\"w\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pip_helper import pip_install\n",
    "\n",
    "pip_install(\n",
    "    \"-q\",\n",
    "    \"torch>=2.1\",\n",
    "    \"torchaudio\",\n",
    "    \"einops\",\n",
    "    \"transformers>=4.46.1\",\n",
    "    \"loguru\",\n",
    "    \"inflect\",\n",
    "    \"pesq\",\n",
    "    \"torchcrepe\",\n",
    "    \"natsort\",\n",
    "    \"polars\",\n",
    "    \"uroman\",\n",
    "    \"mecab-python3\",\n",
    "    \"unidic-lite\",\n",
    "    \"--extra-index-url\",\n",
    "    \"https://download.pytorch.org/whl/cpu\",\n",
    ")\n",
    "pip_install(\n",
    "    \"-q\",\n",
    "    \"gradio>=4.19\",\n",
    "    \"openvino>=2024.4.0\",\n",
    "    \"tqdm\",\n",
    "    \"pyyaml\",\n",
    "    \"librosa\",\n",
    "    \"soundfile\",\n",
    "    \"nncf\",\n",
    ")\n",
    "pip_install(\"-q\", \"git+https://github.com/huggingface/optimum-intel.git\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cpu\")\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    pip_install(\"-q\", \"numpy<2.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"outetts-text-to-speech.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmd_helper import clone_repo\n",
    "\n",
    "repo_path = clone_repo(\"https://github.com/edwko/OuteTTS.git\")\n",
    "\n",
    "interface_path = repo_path / \"outetts/version/v1/interface.py\"\n",
    "\n",
    "updated_version = interface_path.exists()\n",
    "\n",
    "if not updated_version:\n",
    "    interface_pth = repo_path / \"outetts/v0_1/interface.py\"\n",
    "orig_interface_path = interface_path.parent / \"_orig_interface.py\"\n",
    "\n",
    "if not updated_version and not orig_interface_path.exists():\n",
    "    interface_path.rename(orig_interface_path)\n",
    "    # sounddevice requires to install manually additional libraries, as we do not plan to use it for audio playing\n",
    "    # move it closer to its usage for avoid errors\n",
    "    with orig_interface_path.open(\"r\") as in_file:\n",
    "        content = in_file.read()\n",
    "        upd_content = content.replace(\"import sounddevice as sd\", \"\")\n",
    "        upd_content = upd_content.replace(\"sd.play\", \"import sounddevice as sd\\n        sd.play\")\n",
    "    with interface_path.open(\"w\") as out_file:\n",
    "        out_file.write(upd_content)\n",
    "\n",
    "%pip install -q {repo_path} --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "\n",
    " OpenVINO supports PyTorch models via conversion to OpenVINO Intermediate Representation format.  For convenience, we will use OpenVINO integration with HuggingFace Optimum. ü§ó [Optimum Intel](https://huggingface.co/docs/optimum/intel/index) is the interface between the ü§ó Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.\n",
    "\n",
    "Among other use cases, Optimum Intel provides a simple interface to optimize your Transformers and Diffusers models, convert them to the OpenVINO Intermediate Representation (IR) format and run inference using OpenVINO Runtime. `optimum-cli` provides command line interface for model conversion and optimization. \n",
    "\n",
    "General command format:\n",
    "\n",
    "```bash\n",
    "optimum-cli export openvino --model <model_id_or_path> --task <task> <output_dir>\n",
    "```\n",
    "\n",
    "where task is task to export the model for, if not specified, the task will be auto-inferred based on the model. You can find a mapping between tasks and model classes in Optimum TaskManager [documentation](https://huggingface.co/docs/optimum/exporters/task_manager). Additionally, you can specify weights compression using `--weight-format` argument with one of following options: `fp32`, `fp16`, `int8` and `int4`. Fro int8 and int4 [nncf](https://github.com/openvinotoolkit/nncf) will be used for  weight compression. More details about model export provided in [Optimum Intel documentation](https://huggingface.co/docs/optimum/intel/openvino/export#export-your-model).\n",
    "\n",
    "As OuteTTS utilizes pure language modeling approach, model conversion process remains the same like conversion LLaMa models family for text generation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmd_helper import optimum_cli\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"OuteAI/OuteTTS-0.1-350M\"\n",
    "model_dir = Path(model_id.split(\"/\")[-1] + \"-ov\")\n",
    "\n",
    "if not model_dir.exists():\n",
    "    optimum_cli(model_id, model_dir, additional_args={\"task\": \"text-generation-with-past\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model inference\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "\n",
    "OpenVINO integration with Optimum Intel provides ready-to-use API for model inference that can be used for smooth integration with transformers-based solutions. For loading model, we will use `OVModelForCausalLM` class that have compatible interface with Transformers LLaMa implementation. For loading a model, `from_pretrained` method should be used. It accepts path to the model directory or model_id from HuggingFace hub (if model is not converted to OpenVINO format, conversion will be triggered automatically). Additionally, we can provide an inference device, quantization config (if model has not been quantized yet) and device-specific OpenVINO Runtime configuration. More details about model inference with Optimum Intel can be found in [documentation](https://huggingface.co/docs/optimum/intel/openvino/inference). We will use `OVModelForCausalLM` as replacement of original `AutoModelForCausalLM` in `InterfaceHF`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget(exclude=[\"NPU\"])\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_outetts_helper import InterfaceOV, OVHFModel  # noqa: F401\n",
    "\n",
    "# Uncomment these lines to see pipeline details\n",
    "# ??InterfaceOV\n",
    "# ??OVHFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = InterfaceOV(model_dir, device.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-Speech generation\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "\n",
    "Now let's see model in action. Providing input text to `generate` method of interface, model returns tensor that represents output audio with random speaker characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tts_output = interface.generate(text=\"Hello, I'm working!\", temperature=0.1, repetition_penalty=1.1, max_length=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "def play(data, rate=None):\n",
    "    ipd.display(ipd.Audio(data, rate=rate))\n",
    "\n",
    "\n",
    "play(tts_output.audio[0].numpy(), rate=tts_output.sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-Speech generation with Voice Cloning\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "\n",
    "Additionally, we can specify reference voice for generation by providing reference audio and transcript for it. `interface.create_speaker` processes reference audio and text to set of features used for audio description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import download_file\n",
    "\n",
    "ref_audio_url = \"https://huggingface.co/OuteAI/OuteTTS-0.1-350M/resolve/main/samples/2.wav\"\n",
    "file_path = Path(\"2.wav\")\n",
    "\n",
    "if not file_path.exists():\n",
    "    file_path = download_file(ref_audio_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = interface.create_speaker(file_path, \"Hello, I can speak pretty well, but sometimes I make some mistakes.\")\n",
    "\n",
    "# Save the speaker to a file\n",
    "interface.save_speaker(speaker, \"speaker.pkl\")\n",
    "\n",
    "# Load the speaker from a file\n",
    "speaker = interface.load_speaker(\"speaker.pkl\")\n",
    "\n",
    "# Generate TTS with the custom voice\n",
    "cloned_output = interface.generate(\n",
    "    text=\"This is a cloned voice speaking\",\n",
    "    speaker=speaker,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.1,\n",
    "    max_length=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(cloned_output.audio[0].numpy(), rate=cloned_output.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf/) enables post-training quantization by adding the quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers. The framework is designed so that modifications to your original training code are minor.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a calibration dataset for quantization.\n",
    "2. Run `nncf.quantize` to obtain quantized model.\n",
    "3. Serialize the `INT8` model.\n",
    "\n",
    "Note: Quantization is a time and memory-consuming operation. Running the quantization code below may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import quantization_widget\n",
    "\n",
    "to_quantize = quantization_widget()\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare calibration dataset\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "The first step is to prepare calibration datasets for quantization. We will utilize [Filtered LibriTTS-R](https://huggingface.co/datasets/parler-tts/libritts_r_filtered) dataset as it was used to train the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "libritts = load_dataset(\"parler-tts/libritts_r_filtered\", \"clean\", split=\"test.clean\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize model\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "Below we run the quantize function which calls `nncf.quantize` on the OpenVINO IR model and collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import nncf\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "def transform_fn(item, interface):\n",
    "    text_normalized = item[\"text_normalized\"]\n",
    "    prompt = interface.prompt_processor.get_completion_prompt(text_normalized, interface.language, None)\n",
    "    encoded = interface.prompt_processor.tokenizer(prompt, return_tensors=\"np\")\n",
    "\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    attention_mask = encoded[\"attention_mask\"]\n",
    "    inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    position_ids = np.cumsum(attention_mask, axis=1) - 1\n",
    "    position_ids[attention_mask == 0] = 1\n",
    "    inputs[\"position_ids\"] = position_ids\n",
    "\n",
    "    batch_size = input_ids.shape[0]\n",
    "    inputs[\"beam_idx\"] = np.arange(batch_size, dtype=int)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "hf_model = OVHFModel(model_dir, device.value).model\n",
    "dataset = nncf.Dataset(libritts, partial(transform_fn, interface=interface))\n",
    "\n",
    "quantized_model = nncf.quantize(\n",
    "    hf_model.model,\n",
    "    dataset,\n",
    "    preset=nncf.QuantizationPreset.MIXED,\n",
    "    model_type=nncf.ModelType.TRANSFORMER,\n",
    "    ignored_scope=nncf.IgnoredScope(\n",
    "        patterns=[\n",
    "            # We need to use ignored scope for this pattern to generate the most efficient model\n",
    "            \"__module.model.layers.*.self_attn/aten::scaled_dot_product_attention/ScaledDotProductAttention\"\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "hf_model.model = quantized_model\n",
    "model_dir_quantized = Path(f\"{model_dir}_quantized\")\n",
    "hf_model.save_pretrained(model_dir_quantized)\n",
    "interface.prompt_processor.tokenizer.save_pretrained(model_dir_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying quantized model execution\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "In order to verify the quality of the quantized model, we will generate outputs based on the same texts and speakers used for the non-quantized model.\n",
    "First, we will save the quantized model and recreate the pipelines for validation.\n",
    "Then we will generate the outputs and try to compare them with the previously obtained outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "interface_quantized = InterfaceOV(model_dir_quantized, device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "tts_output_quantized = interface_quantized.generate(text=\"Hello, I'm working!\", temperature=0.1, repetition_penalty=1.1, max_length=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "# Non-quantized model output:\n",
    "play(tts_output.audio[0].numpy(), rate=tts_output.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "# Quantized model output:\n",
    "play(tts_output_quantized.audio[0].numpy(), rate=tts_output_quantized.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "speaker_quantized = interface_quantized.load_speaker(\"speaker.pkl\")\n",
    "cloned_output_quantized = interface_quantized.generate(\n",
    "    text=\"This is a cloned voice speaking\",\n",
    "    speaker=speaker,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.1,\n",
    "    max_length=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "# Non-quantized model output:\n",
    "play(cloned_output.audio[0].numpy(), rate=cloned_output.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "# Quantized model output:\n",
    "play(cloned_output_quantized.audio[0].numpy(), rate=cloned_output_quantized.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing model performance\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "def calculate_inference_time(interface, dataset, limit):\n",
    "    inference_time = []\n",
    "    for i, item in tqdm.tqdm(enumerate(dataset), total=limit):\n",
    "        if i > limit: break\n",
    "        start = time.perf_counter()\n",
    "        _ = interface.generate(\n",
    "            text=item[\"text_normalized\"],\n",
    "            max_length=256,\n",
    "            additional_gen_config={\n",
    "                \"pad_token_id\": interface.prompt_processor.tokenizer.eos_token_id\n",
    "            }\n",
    "        )\n",
    "        end = time.perf_counter()\n",
    "        delta = end - start\n",
    "        inference_time.append(delta)\n",
    "    return np.median(inference_time)\n",
    "\n",
    "interface = InterfaceOV(model_dir, device.value)\n",
    "limit = 25\n",
    "\n",
    "fp_inference_time = calculate_inference_time(interface, libritts, limit)\n",
    "print(f\"Original model generate time: {fp_inference_time}\")\n",
    "\n",
    "interface_quantized = InterfaceOV(model_dir_quantized, device.value)\n",
    "int_inference_time = calculate_inference_time(interface_quantized, libritts, limit)\n",
    "print(f\"Quantized model generate time: {int_inference_time}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive demo\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "quantized_model_present = model_dir_quantized.exists()\n",
    "\n",
    "use_quantized_model = widgets.Checkbox(\n",
    "    value=True if quantized_model_present else False,\n",
    "    description=\"Use quantized model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "use_quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_helper import make_demo\n",
    "\n",
    "if use_quantized_model:\n",
    "    demo_interface = InterfaceOV(model_dir_quantized, device.value)\n",
    "else:\n",
    "    demo_interface = InterfaceOV(model_dir, device.value)\n",
    "\n",
    "demo = make_demo(demo_interface)\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/user-attachments/assets/2c667441-c32b-4cb0-8c28-390120b917b3",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [
     "Transformers"
    ],
    "tasks": [
     "Text-to-Audio",
     "Text-to-Speech"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "34dd5fcc238d47b381faaf57c4533034": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b3d79e47520c41fbad29c3279cbe6aa2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c9c530c48b164157a29d10cd1ccc0d93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "AUTO"
       ],
       "description": "Device:",
       "index": 1,
       "layout": "IPY_MODEL_34dd5fcc238d47b381faaf57c4533034",
       "style": "IPY_MODEL_b3d79e47520c41fbad29c3279cbe6aa2"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
