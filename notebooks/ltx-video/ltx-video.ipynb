{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80653d8c-3eb8-4a88-b23f-c31537e82b08",
   "metadata": {},
   "source": [
    "# LTX Video and OpenVINO™\n",
    "\n",
    "[LTX-Video](https://github.com/Lightricks/LTX-Video) is a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32×32×8 pixels per token, enabled by relocating the patchifying operation from the transformer’s input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal selfattention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, this VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. The model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities\n",
    "trained simultaneously.\n",
    "\n",
    "In this example we show how to convert text-to-video pipeline in OpenVINO format and run inference. An additional part demonstrates how to run optimization with [NNCF](https://github.com/openvinotoolkit/nncf/) to speed up pipeline.\n",
    "\n",
    "| | | | |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| ![example1](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main/docs/_static/ltx-video_example_00001.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with long brown hair and light skin smiles at another woman...</summary>A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.</details> | ![example2](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00002.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman walks away from a white Jeep parked on a city street at night...</summary>A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.</details> | ![example3](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00003.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with blonde hair styled up, wearing a black dress...</summary>A woman with blonde hair styled up, wearing a black dress with sequins and pearl earrings, looks down with a sad expression on her face. The camera remains stationary, focused on the woman's face. The lighting is dim, casting soft shadows on her face. The scene appears to be from a movie or TV show.</details> | ![example4](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00004.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>The camera pans over a snow-covered mountain range...</summary>The camera pans over a snow-covered mountain range, revealing a vast expanse of snow-capped peaks and valleys.The mountains are covered in a thick layer of snow, with some areas appearing almost white while others have a slightly darker, almost grayish hue. The peaks are jagged and irregular, with some rising sharply into the sky while others are more rounded. The valleys are deep and narrow, with steep slopes that are also covered in snow. The trees in the foreground are mostly bare, with only a few leaves remaining on their branches. The sky is overcast, with thick clouds obscuring the sun. The overall impression is one of peace and tranquility, with the snow-covered mountains standing as a testament to the power and beauty of nature.</details> |\n",
    "| ![example5](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00005.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with light skin, wearing a blue jacket and a black hat...</summary>A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.</details> | ![example6](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00006.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A man in a dimly lit room talks on a vintage telephone...</summary>A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.</details> | ![example7](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00007.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A prison guard unlocks and opens a cell door...</summary>A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.</details> | ![example8](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00008.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with blood on her face and a white tank top...</summary>A woman with blood on her face and a white tank top looks down and to her right, then back up as she speaks. She has dark hair pulled back, light skin, and her face and chest are covered in blood. The camera angle is a close-up, focused on the woman's face and upper torso. The lighting is dim and blue-toned, creating a somber and intense atmosphere. The scene appears to be from a movie or TV show.</details> |\n",
    "| ![example9](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00009.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A man with graying hair, a beard, and a gray shirt...</summary>A man with graying hair, a beard, and a gray shirt looks down and to his right, then turns his head to the left. The camera angle is a close-up, focused on the man's face. The lighting is dim, with a greenish tint. The scene appears to be real-life footage. Step</details> | ![example10](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00010.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A clear, turquoise river flows through a rocky canyon...</summary>A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.</details> | ![example11](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00011.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A man in a suit enters a room and speaks to two women...</summary>A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.</details> | ![example12](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00012.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>The waves crash against the jagged rocks of the shoreline...</summary>The waves crash against the jagged rocks of the shoreline, sending spray high into the air.The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.</details> |\n",
    "| ![example13](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00013.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>The camera pans across a cityscape of tall buildings...</summary>The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.</details> | ![example14](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00014.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A man walks towards a window, looks out, and then turns around...</summary>A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.</details> | ![example15](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00015.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>Two police officers in dark blue uniforms and matching hats...</summary>Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.</details> | ![example16](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00016.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with short brown hair, wearing a maroon sleeveless top...</summary>A woman with short brown hair, wearing a maroon sleeveless top and a silver necklace, walks through a room while talking, then a woman with pink hair and a white shirt appears in the doorway and yells. The first woman walks from left to right, her expression serious; she has light skin and her eyebrows are slightly furrowed. The second woman stands in the doorway, her mouth open in a yell; she has light skin and her eyes are wide. The room is dimly lit, with a bookshelf visible in the background. The camera follows the first woman as she walks, then cuts to a close-up of the second woman's face. The scene is captured in real-life footage.</details> |\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Load and run the original model](#Load-the-original-model)\n",
    "- [Convert the model to OpenVINO IR](#Convert-the-model-to-OpenVINO-IR)\n",
    "- [Compiling the model](#Compiling-the-model)\n",
    "- [Run OpenVINO model inference](#Run-OpenVINO-model-inference)\n",
    "- [Quantization](#Quantization)\n",
    "    - [Prepare calibration dataset](#Prepare-calibration-dataset)\n",
    "    - [Run Quantization](#Run-Quantization)\n",
    "    - [Run Quantized OpenVINO model inference](#Run-Quantized-OpenVINO-model-inference)\n",
    "    - [Compare Transformer File Size](#Compare-Transformer-File-Size)\n",
    "    - [Compare inference time of the FP16 IR and quantized models](#Compare-inference-time-of-the-FP16-IR-and-quantized-models)\n",
    "- [Interactive inference](#Interactive-inference)\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/ltx-video/ltx-video.ipynb\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab89e7d-5f42-49ee-b082-eabdccd87694",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b837c-bccb-4844-9a5b-e654a40a87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \"torch>=2.1.0\" \"torchvision>=0.16\" \"diffusers>=0.28.2\" \"transformers>=4.44.2\" \"sentencepiece>=0.1.96\" \"huggingface-hub~=0.25.2\" \"einops\" \"accelerate\" \"matplotlib\" \"imageio[ffmpeg]\" \"nncf>=2.15\" \"gradio>=4.26\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install --pre -qU \"openvino>=2025.0.0\" --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab7ea1-a769-4904-a417-eb95bfbb8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "\n",
    "if not Path(\"ov_ltx_video_helper.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/ltx-video/ov_ltx_video_helper.py\",\n",
    "    )\n",
    "    open(\"ov_ltx_video_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"ltx-video.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ce406-e80c-4bd1-8cfa-2d8ac29cdd94",
   "metadata": {},
   "source": [
    "### Load the original model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ed36b-6327-4550-aa5b-50e71a667e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import LTXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "pipe = LTXPipeline.from_pretrained(\"Lightricks/LTX-Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25d40f-c498-427e-8f15-275ba4501777",
   "metadata": {},
   "source": [
    "### Convert the model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    " \n",
    "OpenVINO supports PyTorch models via conversion to OpenVINO Intermediate Representation (IR). [OpenVINO model conversion API](https://docs.openvino.ai/2024/openvino-workflow/model-preparation.html#convert-a-model-with-python-convert-model) should be used for these purposes. `ov.convert_model` function accepts original PyTorch model instance and example input for tracing and returns `ov.Model` representing this model in OpenVINO framework. Converted model can be used for saving on disk using `ov.save_model` function or directly loading on device using `core.complie_model`.\n",
    "\n",
    "`ov_ltx_video_helper.py` script contains helper function for models downloading and models conversion, please check its content if you interested in conversion details. Note that we delete the original models after conversion from pipeline to free memory.\n",
    "\n",
    "LTX Video text-to-video pipeline consists of 3 models: `Text Encoder` converts input text into embeddings, `Transformer` processes these embeddings to generate latents from noise step by step, `VAEDecoder` performs the last denoising step in conjunction with converting latents to pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f819f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_ltx_video_helper import convert_text_encoder, convert_transformer, convert_vae_decoder\n",
    "\n",
    "\n",
    "# Uncomment line below to see model conversion code (replace to convert_transformer and convert_vae_decoder to see them too)\n",
    "# convert_text_encoder??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f109f7-19c1-437b-bcc2-d3881e21c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "MODEL_DIR = Path(\"models\")\n",
    "\n",
    "\n",
    "TEXT_ENCODER_PATH = MODEL_DIR / \"text_encoder_ir.xml\"\n",
    "TRANSFORMER_OV_PATH = MODEL_DIR / \"transformer_ir.xml\"\n",
    "VAE_DECODER_PATH = MODEL_DIR / \"vae_ir.xml\"\n",
    "\n",
    "text_encoder_dtype = pipe.text_encoder.dtype\n",
    "transformer_config = pipe.transformer.config\n",
    "vae_config = pipe.vae.config\n",
    "vae_latents_mean = pipe.vae.latents_mean\n",
    "vae_latents_std = pipe.vae.latents_std\n",
    "\n",
    "convert_text_encoder(pipe.text_encoder, TEXT_ENCODER_PATH)\n",
    "del pipe.text_encoder\n",
    "convert_transformer(pipe.transformer, TRANSFORMER_OV_PATH)\n",
    "del pipe.transformer\n",
    "convert_vae_decoder(pipe.vae, VAE_DECODER_PATH)\n",
    "del pipe.vae\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b28cd-775a-4248-901d-83c7c56601bb",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2d90cc-3a12-486a-9066-1b190e59d92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09a533e9164424f8bdbb8b086b86d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "from notebook_utils import device_widget\n",
    "\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ab305",
   "metadata": {},
   "source": [
    "`ov_catvton_helper.py` provides wrapper classes that wrap the compiled models to keep the original interface. Note that all of wrapper classes return `torch.Tensor`s instead of `np.array`s. Then we insert wrappers instances in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77e44184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_ltx_video_helper import TextEncoderWrapper, ConvTransformerWrapper, VAEWrapper\n",
    "\n",
    "\n",
    "# Uncomment the line below to see the wrapper class code (replace to ConvTransformerWrapper and VAEWrapper to see them too)\n",
    "# TextEncoderWrapper??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96fa82-4a2e-44a7-ae79-91a728ecdc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_transformer = core.compile_model(TRANSFORMER_OV_PATH, device.value)\n",
    "compiled_vae = core.compile_model(VAE_DECODER_PATH, device.value)\n",
    "compiled_text_encoder = core.compile_model(TEXT_ENCODER_PATH, device.value)\n",
    "\n",
    "pipe.__dict__[\"_internal_dict\"][\"_execution_device\"] = pipe._execution_device  # this is to avoid some problem that can occur in the pipeline\n",
    "\n",
    "pipe.register_modules(\n",
    "    text_encoder=TextEncoderWrapper(compiled_text_encoder, text_encoder_dtype),\n",
    "    transformer=ConvTransformerWrapper(compiled_transformer, transformer_config),\n",
    "    vae=VAEWrapper(compiled_vae, vae_config, vae_latents_mean, vae_latents_std),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c588b5-ccea-4006-acf9-42cd0482b004",
   "metadata": {},
   "source": [
    "### Run OpenVINO model inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2419b43",
   "metadata": {},
   "source": [
    "[General tips](https://huggingface.co/Lightricks/LTX-Video#general-tips):\n",
    "- The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the input is not satisfied to the described conditions, the input will be padded with -1 and then cropped to the desired resolution and number of frames.\n",
    "- The model works best on resolutions under 720 x 1280 and number of frames below 257.\n",
    "- Prompts should be in English. The more elaborate the better. Good prompt looks like The turquoise waves crash against the dark, jagged rocks of the shore, sending white foam spraying into the air. The scene is dominated by the stark contrast between the bright blue water and the dark, almost black rocks. The water is a clear, turquoise color, and the waves are capped with white foam. The rocks are dark and jagged, and they are covered in patches of green moss. The shore is lined with lush green vegetation, including trees and bushes. In the background, there are rolling hills covered in dense forest. The sky is cloudy, and the light is dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f504564-6357-483b-b461-b46c3b522576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/sd-xl/openvino_notebooks/venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `_execution_device` directly via 'LTXPipeline' object attribute is deprecated. Please access '_execution_device' over 'LTXPipeline's config object instead, e.g. 'scheduler.config._execution_device'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df21d01f19c248b5935844c9613a83d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'output_ov.mp4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.\"\n",
    "negative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    width=704,\n",
    "    height=480,\n",
    "    num_frames=24,\n",
    "    num_inference_steps=40,\n",
    "    generator=generator,\n",
    "    guidance_scale=3,\n",
    ").frames[0]\n",
    "export_to_video(video, \"output_ov.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0830ee59-9257-4012-97a4-03a83b967687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"output_ov.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"output_ov.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb36ed-2f03-4047-9292-25735ade36cd",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf/) enables post-training quantization by adding quantization layers into model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers. Quantized operations are executed in `INT8` instead of `FP32`/`FP16` making model inference faster.\n",
    "\n",
    "According to `LTX Video Model` structure, the transformer model takes up significant portion of the overall pipeline execution time. Now we will show you how to optimize the transformer part using [NNCF](https://github.com/openvinotoolkit/nncf/) to reduce computation cost and speed up the pipeline. Quantizing the rest of the LTX Video pipeline does not significantly improve inference performance but can lead to a substantial degradation of accuracy.\n",
    "\n",
    "For the transformer model we apply quantization in hybrid mode which means that we quantize: (1) weights of MatMul layers and (2) activations of other layers. The steps are the following:\n",
    "\n",
    "1. Create a calibration dataset for quantization.\n",
    "2. Collect operations with weights.\n",
    "3. Run `nncf.compress_model()` to compress only the model weights.\n",
    "4. Run `nncf.quantize()` on the compressed model with weighted operations ignored by providing `ignored_scope` parameter.\n",
    "5. Save the `INT8` model using `openvino.save_model()` function.\n",
    "\n",
    "Please select below whether you would like to run quantization to improve model inference speed.\n",
    "\n",
    "> **NOTE**: Quantization is time and memory consuming operation. Running quantization code below may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a4b252-19ca-4874-a31c-4f8557847832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e9f4a6f7354f0286ab0c3a75a7066f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Quantization')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import quantization_widget\n",
    "\n",
    "to_quantize = quantization_widget()\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13d968f-fb2a-4247-9377-1596147962a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch `skip_kernel_extension` module\n",
    "import requests\n",
    "\n",
    "if not Path(\"skip_kernel_extension.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    "    )\n",
    "    open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "\n",
    "int8_pipe = None\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb723f4-adcb-499a-b073-db37323acbdd",
   "metadata": {},
   "source": [
    "### Prepare calibration dataset\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c53ab61a-36d6-4b42-9b2b-bb7b4b4da968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data collection started\n",
      "Calibration data collection finished\n"
     ]
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from ltx_video_quantization_helper import TRANSFORMER_INT8_PATH, collect_calibration_data\n",
    "\n",
    "\n",
    "if not TRANSFORMER_INT8_PATH.exists():\n",
    "    calibration_dataset_size = 200\n",
    "    print(\"Calibration data collection started\")\n",
    "    transformer_calibration_data = collect_calibration_data(\n",
    "        pipe,\n",
    "        calibration_dataset_size=calibration_dataset_size,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=5\n",
    "    )\n",
    "    print(\"Calibration data collection finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53213338-023d-4046-a99f-d3184e66c3d1",
   "metadata": {},
   "source": [
    "### Run Quantization\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3aa658d-7e97-495e-a323-8af34a4df53f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│ Weight compression mode   │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│ int8_asym                 │ 100% (287 / 287)            │ 100% (287 / 287)                       │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a260dc2b0024bfc98498e8f58a44351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:287 ignored nodes were found by names in the NNCFGraph\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 10 __module.caption_projection.linear_1/aten::linear/MatMul\n",
      "21 __module.caption_projection.linear_1/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 61 __module.caption_projection.linear_2/aten::linear/MatMul\n",
      "167 __module.caption_projection.linear_2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 104 __module.transformer_blocks.0.attn2.to_k/aten::linear/MatMul\n",
      "239 __module.transformer_blocks.0.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 105 __module.transformer_blocks.0.attn2.to_v/aten::linear/MatMul\n",
      "240 __module.transformer_blocks.0.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 106 __module.transformer_blocks.1.attn2.to_k/aten::linear/MatMul\n",
      "241 __module.transformer_blocks.1.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 107 __module.transformer_blocks.1.attn2.to_v/aten::linear/MatMul\n",
      "242 __module.transformer_blocks.1.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 108 __module.transformer_blocks.10.attn2.to_k/aten::linear/MatMul\n",
      "243 __module.transformer_blocks.10.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 109 __module.transformer_blocks.10.attn2.to_v/aten::linear/MatMul\n",
      "244 __module.transformer_blocks.10.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 110 __module.transformer_blocks.11.attn2.to_k/aten::linear/MatMul\n",
      "245 __module.transformer_blocks.11.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 111 __module.transformer_blocks.11.attn2.to_v/aten::linear/MatMul\n",
      "246 __module.transformer_blocks.11.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 112 __module.transformer_blocks.12.attn2.to_k/aten::linear/MatMul\n",
      "247 __module.transformer_blocks.12.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 113 __module.transformer_blocks.12.attn2.to_v/aten::linear/MatMul\n",
      "248 __module.transformer_blocks.12.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 114 __module.transformer_blocks.13.attn2.to_k/aten::linear/MatMul\n",
      "249 __module.transformer_blocks.13.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 115 __module.transformer_blocks.13.attn2.to_v/aten::linear/MatMul\n",
      "250 __module.transformer_blocks.13.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 116 __module.transformer_blocks.14.attn2.to_k/aten::linear/MatMul\n",
      "251 __module.transformer_blocks.14.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 117 __module.transformer_blocks.14.attn2.to_v/aten::linear/MatMul\n",
      "252 __module.transformer_blocks.14.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 118 __module.transformer_blocks.15.attn2.to_k/aten::linear/MatMul\n",
      "253 __module.transformer_blocks.15.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 119 __module.transformer_blocks.15.attn2.to_v/aten::linear/MatMul\n",
      "254 __module.transformer_blocks.15.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 120 __module.transformer_blocks.16.attn2.to_k/aten::linear/MatMul\n",
      "255 __module.transformer_blocks.16.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 121 __module.transformer_blocks.16.attn2.to_v/aten::linear/MatMul\n",
      "256 __module.transformer_blocks.16.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 122 __module.transformer_blocks.17.attn2.to_k/aten::linear/MatMul\n",
      "257 __module.transformer_blocks.17.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 123 __module.transformer_blocks.17.attn2.to_v/aten::linear/MatMul\n",
      "258 __module.transformer_blocks.17.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 124 __module.transformer_blocks.18.attn2.to_k/aten::linear/MatMul\n",
      "259 __module.transformer_blocks.18.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 125 __module.transformer_blocks.18.attn2.to_v/aten::linear/MatMul\n",
      "260 __module.transformer_blocks.18.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 126 __module.transformer_blocks.19.attn2.to_k/aten::linear/MatMul\n",
      "261 __module.transformer_blocks.19.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 127 __module.transformer_blocks.19.attn2.to_v/aten::linear/MatMul\n",
      "262 __module.transformer_blocks.19.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 128 __module.transformer_blocks.2.attn2.to_k/aten::linear/MatMul\n",
      "263 __module.transformer_blocks.2.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 129 __module.transformer_blocks.2.attn2.to_v/aten::linear/MatMul\n",
      "264 __module.transformer_blocks.2.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 130 __module.transformer_blocks.20.attn2.to_k/aten::linear/MatMul\n",
      "265 __module.transformer_blocks.20.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 131 __module.transformer_blocks.20.attn2.to_v/aten::linear/MatMul\n",
      "266 __module.transformer_blocks.20.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 132 __module.transformer_blocks.21.attn2.to_k/aten::linear/MatMul\n",
      "267 __module.transformer_blocks.21.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 133 __module.transformer_blocks.21.attn2.to_v/aten::linear/MatMul\n",
      "268 __module.transformer_blocks.21.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 134 __module.transformer_blocks.22.attn2.to_k/aten::linear/MatMul\n",
      "269 __module.transformer_blocks.22.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 135 __module.transformer_blocks.22.attn2.to_v/aten::linear/MatMul\n",
      "270 __module.transformer_blocks.22.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 136 __module.transformer_blocks.23.attn2.to_k/aten::linear/MatMul\n",
      "271 __module.transformer_blocks.23.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 137 __module.transformer_blocks.23.attn2.to_v/aten::linear/MatMul\n",
      "272 __module.transformer_blocks.23.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 138 __module.transformer_blocks.24.attn2.to_k/aten::linear/MatMul\n",
      "273 __module.transformer_blocks.24.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 139 __module.transformer_blocks.24.attn2.to_v/aten::linear/MatMul\n",
      "274 __module.transformer_blocks.24.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 140 __module.transformer_blocks.25.attn2.to_k/aten::linear/MatMul\n",
      "275 __module.transformer_blocks.25.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 141 __module.transformer_blocks.25.attn2.to_v/aten::linear/MatMul\n",
      "276 __module.transformer_blocks.25.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 142 __module.transformer_blocks.26.attn2.to_k/aten::linear/MatMul\n",
      "277 __module.transformer_blocks.26.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 143 __module.transformer_blocks.26.attn2.to_v/aten::linear/MatMul\n",
      "278 __module.transformer_blocks.26.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 144 __module.transformer_blocks.27.attn2.to_k/aten::linear/MatMul\n",
      "279 __module.transformer_blocks.27.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 145 __module.transformer_blocks.27.attn2.to_v/aten::linear/MatMul\n",
      "280 __module.transformer_blocks.27.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 146 __module.transformer_blocks.3.attn2.to_k/aten::linear/MatMul\n",
      "281 __module.transformer_blocks.3.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 147 __module.transformer_blocks.3.attn2.to_v/aten::linear/MatMul\n",
      "282 __module.transformer_blocks.3.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 148 __module.transformer_blocks.4.attn2.to_k/aten::linear/MatMul\n",
      "283 __module.transformer_blocks.4.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 149 __module.transformer_blocks.4.attn2.to_v/aten::linear/MatMul\n",
      "284 __module.transformer_blocks.4.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 150 __module.transformer_blocks.5.attn2.to_k/aten::linear/MatMul\n",
      "285 __module.transformer_blocks.5.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 151 __module.transformer_blocks.5.attn2.to_v/aten::linear/MatMul\n",
      "286 __module.transformer_blocks.5.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 152 __module.transformer_blocks.6.attn2.to_k/aten::linear/MatMul\n",
      "287 __module.transformer_blocks.6.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 153 __module.transformer_blocks.6.attn2.to_v/aten::linear/MatMul\n",
      "288 __module.transformer_blocks.6.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 154 __module.transformer_blocks.7.attn2.to_k/aten::linear/MatMul\n",
      "289 __module.transformer_blocks.7.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 155 __module.transformer_blocks.7.attn2.to_v/aten::linear/MatMul\n",
      "290 __module.transformer_blocks.7.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 156 __module.transformer_blocks.8.attn2.to_k/aten::linear/MatMul\n",
      "291 __module.transformer_blocks.8.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 157 __module.transformer_blocks.8.attn2.to_v/aten::linear/MatMul\n",
      "292 __module.transformer_blocks.8.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 158 __module.transformer_blocks.9.attn2.to_k/aten::linear/MatMul\n",
      "293 __module.transformer_blocks.9.attn2.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 159 __module.transformer_blocks.9.attn2.to_v/aten::linear/MatMul\n",
      "294 __module.transformer_blocks.9.attn2.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 9 __module.proj_in/aten::linear/MatMul\n",
      "20 __module.proj_in/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 475 __module.time_embed.emb.timestep_embedder.linear_1/aten::linear/MatMul\n",
      "778 __module.time_embed.emb.timestep_embedder.linear_1/aten::linear/Add\n",
      "1052 __module.time_embed.emb.timestep_embedder.act/aten::silu/Swish\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1364 __module.time_embed.emb.timestep_embedder.linear_2/aten::linear/MatMul\n",
      "1647 __module.time_embed.emb.timestep_embedder.linear_2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2181 __module.time_embed.linear/aten::linear/MatMul\n",
      "2508 __module.time_embed.linear/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 296 __module.transformer_blocks.0.attn1.to_k/aten::linear/MatMul\n",
      "463 __module.transformer_blocks.0.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 297 __module.transformer_blocks.0.attn1.to_q/aten::linear/MatMul\n",
      "464 __module.transformer_blocks.0.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 298 __module.transformer_blocks.0.attn1.to_v/aten::linear/MatMul\n",
      "465 __module.transformer_blocks.0.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2169 __module.transformer_blocks.0.attn1.to_out.0/aten::linear/MatMul\n",
      "2498 __module.transformer_blocks.0.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 59 __module.transformer_blocks.0.attn2.to_q/aten::linear/MatMul\n",
      "163 __module.transformer_blocks.0.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 481 __module.transformer_blocks.0.attn2.to_out.0/aten::linear/MatMul\n",
      "781 __module.transformer_blocks.0.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 770 __module.transformer_blocks.0.ff.net.0.proj/aten::linear/MatMul\n",
      "1039 __module.transformer_blocks.0.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1636 __module.transformer_blocks.0.ff.net.2/aten::linear/MatMul\n",
      "1891 __module.transformer_blocks.0.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1041 __module.transformer_blocks.1.attn1.to_k/aten::linear/MatMul\n",
      "1355 __module.transformer_blocks.1.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1042 __module.transformer_blocks.1.attn1.to_q/aten::linear/MatMul\n",
      "1356 __module.transformer_blocks.1.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1043 __module.transformer_blocks.1.attn1.to_v/aten::linear/MatMul\n",
      "1357 __module.transformer_blocks.1.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3105 __module.transformer_blocks.1.attn1.to_out.0/aten::linear/MatMul\n",
      "3375 __module.transformer_blocks.1.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 473 __module.transformer_blocks.1.attn2.to_q/aten::linear/MatMul\n",
      "774 __module.transformer_blocks.1.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 482 __module.transformer_blocks.1.attn2.to_out.0/aten::linear/MatMul\n",
      "782 __module.transformer_blocks.1.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1081 __module.transformer_blocks.1.ff.net.0.proj/aten::linear/MatMul\n",
      "1419 __module.transformer_blocks.1.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1931 __module.transformer_blocks.1.ff.net.2/aten::linear/MatMul\n",
      "2210 __module.transformer_blocks.1.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1162 __module.transformer_blocks.2.attn1.to_k/aten::linear/MatMul\n",
      "1490 __module.transformer_blocks.2.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1163 __module.transformer_blocks.2.attn1.to_q/aten::linear/MatMul\n",
      "1491 __module.transformer_blocks.2.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1164 __module.transformer_blocks.2.attn1.to_v/aten::linear/MatMul\n",
      "1492 __module.transformer_blocks.2.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3250 __module.transformer_blocks.2.attn1.to_out.0/aten::linear/MatMul\n",
      "3433 __module.transformer_blocks.2.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1363 __module.transformer_blocks.2.attn2.to_q/aten::linear/MatMul\n",
      "1646 __module.transformer_blocks.2.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 493 __module.transformer_blocks.2.attn2.to_out.0/aten::linear/MatMul\n",
      "793 __module.transformer_blocks.2.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1166 __module.transformer_blocks.2.ff.net.0.proj/aten::linear/MatMul\n",
      "1493 __module.transformer_blocks.2.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2027 __module.transformer_blocks.2.ff.net.2/aten::linear/MatMul\n",
      "2328 __module.transformer_blocks.2.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1233 __module.transformer_blocks.3.attn1.to_k/aten::linear/MatMul\n",
      "1551 __module.transformer_blocks.3.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1234 __module.transformer_blocks.3.attn1.to_q/aten::linear/MatMul\n",
      "1552 __module.transformer_blocks.3.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1235 __module.transformer_blocks.3.attn1.to_v/aten::linear/MatMul\n",
      "1553 __module.transformer_blocks.3.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3321 __module.transformer_blocks.3.attn1.to_out.0/aten::linear/MatMul\n",
      "3478 __module.transformer_blocks.3.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1554 __module.transformer_blocks.3.attn2.to_q/aten::linear/MatMul\n",
      "1831 __module.transformer_blocks.3.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 502 __module.transformer_blocks.3.attn2.to_out.0/aten::linear/MatMul\n",
      "802 __module.transformer_blocks.3.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1238 __module.transformer_blocks.3.ff.net.0.proj/aten::linear/MatMul\n",
      "1555 __module.transformer_blocks.3.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2107 __module.transformer_blocks.3.ff.net.2/aten::linear/MatMul\n",
      "2426 __module.transformer_blocks.3.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1241 __module.transformer_blocks.4.attn1.to_k/aten::linear/MatMul\n",
      "1558 __module.transformer_blocks.4.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1242 __module.transformer_blocks.4.attn1.to_q/aten::linear/MatMul\n",
      "1559 __module.transformer_blocks.4.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1243 __module.transformer_blocks.4.attn1.to_v/aten::linear/MatMul\n",
      "1560 __module.transformer_blocks.4.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3329 __module.transformer_blocks.4.attn1.to_out.0/aten::linear/MatMul\n",
      "3483 __module.transformer_blocks.4.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1561 __module.transformer_blocks.4.attn2.to_q/aten::linear/MatMul\n",
      "1839 __module.transformer_blocks.4.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 503 __module.transformer_blocks.4.attn2.to_out.0/aten::linear/MatMul\n",
      "803 __module.transformer_blocks.4.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1246 __module.transformer_blocks.4.ff.net.0.proj/aten::linear/MatMul\n",
      "1562 __module.transformer_blocks.4.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2116 __module.transformer_blocks.4.ff.net.2/aten::linear/MatMul\n",
      "2437 __module.transformer_blocks.4.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1249 __module.transformer_blocks.5.attn1.to_k/aten::linear/MatMul\n",
      "1565 __module.transformer_blocks.5.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1250 __module.transformer_blocks.5.attn1.to_q/aten::linear/MatMul\n",
      "1566 __module.transformer_blocks.5.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1251 __module.transformer_blocks.5.attn1.to_v/aten::linear/MatMul\n",
      "1567 __module.transformer_blocks.5.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3337 __module.transformer_blocks.5.attn1.to_out.0/aten::linear/MatMul\n",
      "3488 __module.transformer_blocks.5.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1568 __module.transformer_blocks.5.attn2.to_q/aten::linear/MatMul\n",
      "1847 __module.transformer_blocks.5.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 504 __module.transformer_blocks.5.attn2.to_out.0/aten::linear/MatMul\n",
      "804 __module.transformer_blocks.5.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1254 __module.transformer_blocks.5.ff.net.0.proj/aten::linear/MatMul\n",
      "1569 __module.transformer_blocks.5.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2125 __module.transformer_blocks.5.ff.net.2/aten::linear/MatMul\n",
      "2448 __module.transformer_blocks.5.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1257 __module.transformer_blocks.6.attn1.to_k/aten::linear/MatMul\n",
      "1572 __module.transformer_blocks.6.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1258 __module.transformer_blocks.6.attn1.to_q/aten::linear/MatMul\n",
      "1573 __module.transformer_blocks.6.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1259 __module.transformer_blocks.6.attn1.to_v/aten::linear/MatMul\n",
      "1574 __module.transformer_blocks.6.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3345 __module.transformer_blocks.6.attn1.to_out.0/aten::linear/MatMul\n",
      "3493 __module.transformer_blocks.6.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1575 __module.transformer_blocks.6.attn2.to_q/aten::linear/MatMul\n",
      "1855 __module.transformer_blocks.6.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 505 __module.transformer_blocks.6.attn2.to_out.0/aten::linear/MatMul\n",
      "805 __module.transformer_blocks.6.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1262 __module.transformer_blocks.6.ff.net.0.proj/aten::linear/MatMul\n",
      "1576 __module.transformer_blocks.6.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2134 __module.transformer_blocks.6.ff.net.2/aten::linear/MatMul\n",
      "2459 __module.transformer_blocks.6.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1265 __module.transformer_blocks.7.attn1.to_k/aten::linear/MatMul\n",
      "1579 __module.transformer_blocks.7.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1266 __module.transformer_blocks.7.attn1.to_q/aten::linear/MatMul\n",
      "1580 __module.transformer_blocks.7.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1267 __module.transformer_blocks.7.attn1.to_v/aten::linear/MatMul\n",
      "1581 __module.transformer_blocks.7.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3353 __module.transformer_blocks.7.attn1.to_out.0/aten::linear/MatMul\n",
      "3498 __module.transformer_blocks.7.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1582 __module.transformer_blocks.7.attn2.to_q/aten::linear/MatMul\n",
      "1863 __module.transformer_blocks.7.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 506 __module.transformer_blocks.7.attn2.to_out.0/aten::linear/MatMul\n",
      "806 __module.transformer_blocks.7.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1270 __module.transformer_blocks.7.ff.net.0.proj/aten::linear/MatMul\n",
      "1583 __module.transformer_blocks.7.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2143 __module.transformer_blocks.7.ff.net.2/aten::linear/MatMul\n",
      "2470 __module.transformer_blocks.7.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1273 __module.transformer_blocks.8.attn1.to_k/aten::linear/MatMul\n",
      "1586 __module.transformer_blocks.8.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1274 __module.transformer_blocks.8.attn1.to_q/aten::linear/MatMul\n",
      "1587 __module.transformer_blocks.8.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1275 __module.transformer_blocks.8.attn1.to_v/aten::linear/MatMul\n",
      "1588 __module.transformer_blocks.8.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3361 __module.transformer_blocks.8.attn1.to_out.0/aten::linear/MatMul\n",
      "3503 __module.transformer_blocks.8.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1589 __module.transformer_blocks.8.attn2.to_q/aten::linear/MatMul\n",
      "1871 __module.transformer_blocks.8.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 507 __module.transformer_blocks.8.attn2.to_out.0/aten::linear/MatMul\n",
      "807 __module.transformer_blocks.8.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1278 __module.transformer_blocks.8.ff.net.0.proj/aten::linear/MatMul\n",
      "1590 __module.transformer_blocks.8.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2152 __module.transformer_blocks.8.ff.net.2/aten::linear/MatMul\n",
      "2481 __module.transformer_blocks.8.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1281 __module.transformer_blocks.9.attn1.to_k/aten::linear/MatMul\n",
      "1593 __module.transformer_blocks.9.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1282 __module.transformer_blocks.9.attn1.to_q/aten::linear/MatMul\n",
      "1594 __module.transformer_blocks.9.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1283 __module.transformer_blocks.9.attn1.to_v/aten::linear/MatMul\n",
      "1595 __module.transformer_blocks.9.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3369 __module.transformer_blocks.9.attn1.to_out.0/aten::linear/MatMul\n",
      "3508 __module.transformer_blocks.9.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1596 __module.transformer_blocks.9.attn2.to_q/aten::linear/MatMul\n",
      "1879 __module.transformer_blocks.9.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 508 __module.transformer_blocks.9.attn2.to_out.0/aten::linear/MatMul\n",
      "808 __module.transformer_blocks.9.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1286 __module.transformer_blocks.9.ff.net.0.proj/aten::linear/MatMul\n",
      "1597 __module.transformer_blocks.9.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2161 __module.transformer_blocks.9.ff.net.2/aten::linear/MatMul\n",
      "2492 __module.transformer_blocks.9.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1082 __module.transformer_blocks.10.attn1.to_k/aten::linear/MatMul\n",
      "1420 __module.transformer_blocks.10.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1083 __module.transformer_blocks.10.attn1.to_q/aten::linear/MatMul\n",
      "1421 __module.transformer_blocks.10.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1084 __module.transformer_blocks.10.attn1.to_v/aten::linear/MatMul\n",
      "1422 __module.transformer_blocks.10.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3170 __module.transformer_blocks.10.attn1.to_out.0/aten::linear/MatMul\n",
      "3383 __module.transformer_blocks.10.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1423 __module.transformer_blocks.10.attn2.to_q/aten::linear/MatMul\n",
      "1681 __module.transformer_blocks.10.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 483 __module.transformer_blocks.10.attn2.to_out.0/aten::linear/MatMul\n",
      "783 __module.transformer_blocks.10.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1087 __module.transformer_blocks.10.ff.net.0.proj/aten::linear/MatMul\n",
      "1424 __module.transformer_blocks.10.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1939 __module.transformer_blocks.10.ff.net.2/aten::linear/MatMul\n",
      "2220 __module.transformer_blocks.10.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1090 __module.transformer_blocks.11.attn1.to_k/aten::linear/MatMul\n",
      "1427 __module.transformer_blocks.11.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1091 __module.transformer_blocks.11.attn1.to_q/aten::linear/MatMul\n",
      "1428 __module.transformer_blocks.11.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1092 __module.transformer_blocks.11.attn1.to_v/aten::linear/MatMul\n",
      "1429 __module.transformer_blocks.11.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3178 __module.transformer_blocks.11.attn1.to_out.0/aten::linear/MatMul\n",
      "3388 __module.transformer_blocks.11.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1430 __module.transformer_blocks.11.attn2.to_q/aten::linear/MatMul\n",
      "1689 __module.transformer_blocks.11.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 484 __module.transformer_blocks.11.attn2.to_out.0/aten::linear/MatMul\n",
      "784 __module.transformer_blocks.11.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1095 __module.transformer_blocks.11.ff.net.0.proj/aten::linear/MatMul\n",
      "1431 __module.transformer_blocks.11.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1948 __module.transformer_blocks.11.ff.net.2/aten::linear/MatMul\n",
      "2231 __module.transformer_blocks.11.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1098 __module.transformer_blocks.12.attn1.to_k/aten::linear/MatMul\n",
      "1434 __module.transformer_blocks.12.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1099 __module.transformer_blocks.12.attn1.to_q/aten::linear/MatMul\n",
      "1435 __module.transformer_blocks.12.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1100 __module.transformer_blocks.12.attn1.to_v/aten::linear/MatMul\n",
      "1436 __module.transformer_blocks.12.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3186 __module.transformer_blocks.12.attn1.to_out.0/aten::linear/MatMul\n",
      "3393 __module.transformer_blocks.12.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1437 __module.transformer_blocks.12.attn2.to_q/aten::linear/MatMul\n",
      "1697 __module.transformer_blocks.12.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 485 __module.transformer_blocks.12.attn2.to_out.0/aten::linear/MatMul\n",
      "785 __module.transformer_blocks.12.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1103 __module.transformer_blocks.12.ff.net.0.proj/aten::linear/MatMul\n",
      "1438 __module.transformer_blocks.12.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1957 __module.transformer_blocks.12.ff.net.2/aten::linear/MatMul\n",
      "2242 __module.transformer_blocks.12.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1106 __module.transformer_blocks.13.attn1.to_k/aten::linear/MatMul\n",
      "1441 __module.transformer_blocks.13.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1107 __module.transformer_blocks.13.attn1.to_q/aten::linear/MatMul\n",
      "1442 __module.transformer_blocks.13.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1108 __module.transformer_blocks.13.attn1.to_v/aten::linear/MatMul\n",
      "1443 __module.transformer_blocks.13.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3194 __module.transformer_blocks.13.attn1.to_out.0/aten::linear/MatMul\n",
      "3398 __module.transformer_blocks.13.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1444 __module.transformer_blocks.13.attn2.to_q/aten::linear/MatMul\n",
      "1705 __module.transformer_blocks.13.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 486 __module.transformer_blocks.13.attn2.to_out.0/aten::linear/MatMul\n",
      "786 __module.transformer_blocks.13.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1111 __module.transformer_blocks.13.ff.net.0.proj/aten::linear/MatMul\n",
      "1445 __module.transformer_blocks.13.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1966 __module.transformer_blocks.13.ff.net.2/aten::linear/MatMul\n",
      "2253 __module.transformer_blocks.13.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1114 __module.transformer_blocks.14.attn1.to_k/aten::linear/MatMul\n",
      "1448 __module.transformer_blocks.14.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1115 __module.transformer_blocks.14.attn1.to_q/aten::linear/MatMul\n",
      "1449 __module.transformer_blocks.14.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1116 __module.transformer_blocks.14.attn1.to_v/aten::linear/MatMul\n",
      "1450 __module.transformer_blocks.14.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3202 __module.transformer_blocks.14.attn1.to_out.0/aten::linear/MatMul\n",
      "3403 __module.transformer_blocks.14.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1451 __module.transformer_blocks.14.attn2.to_q/aten::linear/MatMul\n",
      "1713 __module.transformer_blocks.14.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 487 __module.transformer_blocks.14.attn2.to_out.0/aten::linear/MatMul\n",
      "787 __module.transformer_blocks.14.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1119 __module.transformer_blocks.14.ff.net.0.proj/aten::linear/MatMul\n",
      "1452 __module.transformer_blocks.14.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1975 __module.transformer_blocks.14.ff.net.2/aten::linear/MatMul\n",
      "2264 __module.transformer_blocks.14.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1122 __module.transformer_blocks.15.attn1.to_k/aten::linear/MatMul\n",
      "1455 __module.transformer_blocks.15.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1123 __module.transformer_blocks.15.attn1.to_q/aten::linear/MatMul\n",
      "1456 __module.transformer_blocks.15.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1124 __module.transformer_blocks.15.attn1.to_v/aten::linear/MatMul\n",
      "1457 __module.transformer_blocks.15.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3210 __module.transformer_blocks.15.attn1.to_out.0/aten::linear/MatMul\n",
      "3408 __module.transformer_blocks.15.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1458 __module.transformer_blocks.15.attn2.to_q/aten::linear/MatMul\n",
      "1721 __module.transformer_blocks.15.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 488 __module.transformer_blocks.15.attn2.to_out.0/aten::linear/MatMul\n",
      "788 __module.transformer_blocks.15.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1127 __module.transformer_blocks.15.ff.net.0.proj/aten::linear/MatMul\n",
      "1459 __module.transformer_blocks.15.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1984 __module.transformer_blocks.15.ff.net.2/aten::linear/MatMul\n",
      "2275 __module.transformer_blocks.15.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1130 __module.transformer_blocks.16.attn1.to_k/aten::linear/MatMul\n",
      "1462 __module.transformer_blocks.16.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1131 __module.transformer_blocks.16.attn1.to_q/aten::linear/MatMul\n",
      "1463 __module.transformer_blocks.16.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1132 __module.transformer_blocks.16.attn1.to_v/aten::linear/MatMul\n",
      "1464 __module.transformer_blocks.16.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3218 __module.transformer_blocks.16.attn1.to_out.0/aten::linear/MatMul\n",
      "3413 __module.transformer_blocks.16.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1465 __module.transformer_blocks.16.attn2.to_q/aten::linear/MatMul\n",
      "1729 __module.transformer_blocks.16.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 489 __module.transformer_blocks.16.attn2.to_out.0/aten::linear/MatMul\n",
      "789 __module.transformer_blocks.16.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1135 __module.transformer_blocks.16.ff.net.0.proj/aten::linear/MatMul\n",
      "1466 __module.transformer_blocks.16.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1993 __module.transformer_blocks.16.ff.net.2/aten::linear/MatMul\n",
      "2286 __module.transformer_blocks.16.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1138 __module.transformer_blocks.17.attn1.to_k/aten::linear/MatMul\n",
      "1469 __module.transformer_blocks.17.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1139 __module.transformer_blocks.17.attn1.to_q/aten::linear/MatMul\n",
      "1470 __module.transformer_blocks.17.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1140 __module.transformer_blocks.17.attn1.to_v/aten::linear/MatMul\n",
      "1471 __module.transformer_blocks.17.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3226 __module.transformer_blocks.17.attn1.to_out.0/aten::linear/MatMul\n",
      "3418 __module.transformer_blocks.17.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1472 __module.transformer_blocks.17.attn2.to_q/aten::linear/MatMul\n",
      "1737 __module.transformer_blocks.17.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 490 __module.transformer_blocks.17.attn2.to_out.0/aten::linear/MatMul\n",
      "790 __module.transformer_blocks.17.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1143 __module.transformer_blocks.17.ff.net.0.proj/aten::linear/MatMul\n",
      "1473 __module.transformer_blocks.17.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2002 __module.transformer_blocks.17.ff.net.2/aten::linear/MatMul\n",
      "2297 __module.transformer_blocks.17.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1146 __module.transformer_blocks.18.attn1.to_k/aten::linear/MatMul\n",
      "1476 __module.transformer_blocks.18.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1147 __module.transformer_blocks.18.attn1.to_q/aten::linear/MatMul\n",
      "1477 __module.transformer_blocks.18.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1148 __module.transformer_blocks.18.attn1.to_v/aten::linear/MatMul\n",
      "1478 __module.transformer_blocks.18.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3234 __module.transformer_blocks.18.attn1.to_out.0/aten::linear/MatMul\n",
      "3423 __module.transformer_blocks.18.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1479 __module.transformer_blocks.18.attn2.to_q/aten::linear/MatMul\n",
      "1745 __module.transformer_blocks.18.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 491 __module.transformer_blocks.18.attn2.to_out.0/aten::linear/MatMul\n",
      "791 __module.transformer_blocks.18.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1151 __module.transformer_blocks.18.ff.net.0.proj/aten::linear/MatMul\n",
      "1480 __module.transformer_blocks.18.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2011 __module.transformer_blocks.18.ff.net.2/aten::linear/MatMul\n",
      "2308 __module.transformer_blocks.18.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1154 __module.transformer_blocks.19.attn1.to_k/aten::linear/MatMul\n",
      "1483 __module.transformer_blocks.19.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1155 __module.transformer_blocks.19.attn1.to_q/aten::linear/MatMul\n",
      "1484 __module.transformer_blocks.19.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1156 __module.transformer_blocks.19.attn1.to_v/aten::linear/MatMul\n",
      "1485 __module.transformer_blocks.19.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3242 __module.transformer_blocks.19.attn1.to_out.0/aten::linear/MatMul\n",
      "3428 __module.transformer_blocks.19.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1486 __module.transformer_blocks.19.attn2.to_q/aten::linear/MatMul\n",
      "1753 __module.transformer_blocks.19.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 492 __module.transformer_blocks.19.attn2.to_out.0/aten::linear/MatMul\n",
      "792 __module.transformer_blocks.19.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1159 __module.transformer_blocks.19.ff.net.0.proj/aten::linear/MatMul\n",
      "1487 __module.transformer_blocks.19.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2020 __module.transformer_blocks.19.ff.net.2/aten::linear/MatMul\n",
      "2319 __module.transformer_blocks.19.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1169 __module.transformer_blocks.20.attn1.to_k/aten::linear/MatMul\n",
      "1496 __module.transformer_blocks.20.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1170 __module.transformer_blocks.20.attn1.to_q/aten::linear/MatMul\n",
      "1497 __module.transformer_blocks.20.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1171 __module.transformer_blocks.20.attn1.to_v/aten::linear/MatMul\n",
      "1498 __module.transformer_blocks.20.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3257 __module.transformer_blocks.20.attn1.to_out.0/aten::linear/MatMul\n",
      "3438 __module.transformer_blocks.20.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1499 __module.transformer_blocks.20.attn2.to_q/aten::linear/MatMul\n",
      "1768 __module.transformer_blocks.20.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 494 __module.transformer_blocks.20.attn2.to_out.0/aten::linear/MatMul\n",
      "794 __module.transformer_blocks.20.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1174 __module.transformer_blocks.20.ff.net.0.proj/aten::linear/MatMul\n",
      "1500 __module.transformer_blocks.20.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2036 __module.transformer_blocks.20.ff.net.2/aten::linear/MatMul\n",
      "2339 __module.transformer_blocks.20.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1177 __module.transformer_blocks.21.attn1.to_k/aten::linear/MatMul\n",
      "1503 __module.transformer_blocks.21.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1178 __module.transformer_blocks.21.attn1.to_q/aten::linear/MatMul\n",
      "1504 __module.transformer_blocks.21.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1179 __module.transformer_blocks.21.attn1.to_v/aten::linear/MatMul\n",
      "1505 __module.transformer_blocks.21.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3265 __module.transformer_blocks.21.attn1.to_out.0/aten::linear/MatMul\n",
      "3443 __module.transformer_blocks.21.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1506 __module.transformer_blocks.21.attn2.to_q/aten::linear/MatMul\n",
      "1776 __module.transformer_blocks.21.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 495 __module.transformer_blocks.21.attn2.to_out.0/aten::linear/MatMul\n",
      "795 __module.transformer_blocks.21.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1182 __module.transformer_blocks.21.ff.net.0.proj/aten::linear/MatMul\n",
      "1507 __module.transformer_blocks.21.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2045 __module.transformer_blocks.21.ff.net.2/aten::linear/MatMul\n",
      "2350 __module.transformer_blocks.21.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1185 __module.transformer_blocks.22.attn1.to_k/aten::linear/MatMul\n",
      "1510 __module.transformer_blocks.22.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1186 __module.transformer_blocks.22.attn1.to_q/aten::linear/MatMul\n",
      "1511 __module.transformer_blocks.22.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1187 __module.transformer_blocks.22.attn1.to_v/aten::linear/MatMul\n",
      "1512 __module.transformer_blocks.22.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3273 __module.transformer_blocks.22.attn1.to_out.0/aten::linear/MatMul\n",
      "3448 __module.transformer_blocks.22.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1513 __module.transformer_blocks.22.attn2.to_q/aten::linear/MatMul\n",
      "1784 __module.transformer_blocks.22.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 496 __module.transformer_blocks.22.attn2.to_out.0/aten::linear/MatMul\n",
      "796 __module.transformer_blocks.22.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1190 __module.transformer_blocks.22.ff.net.0.proj/aten::linear/MatMul\n",
      "1514 __module.transformer_blocks.22.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2054 __module.transformer_blocks.22.ff.net.2/aten::linear/MatMul\n",
      "2361 __module.transformer_blocks.22.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1193 __module.transformer_blocks.23.attn1.to_k/aten::linear/MatMul\n",
      "1517 __module.transformer_blocks.23.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1194 __module.transformer_blocks.23.attn1.to_q/aten::linear/MatMul\n",
      "1518 __module.transformer_blocks.23.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1195 __module.transformer_blocks.23.attn1.to_v/aten::linear/MatMul\n",
      "1519 __module.transformer_blocks.23.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3281 __module.transformer_blocks.23.attn1.to_out.0/aten::linear/MatMul\n",
      "3453 __module.transformer_blocks.23.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1520 __module.transformer_blocks.23.attn2.to_q/aten::linear/MatMul\n",
      "1792 __module.transformer_blocks.23.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 497 __module.transformer_blocks.23.attn2.to_out.0/aten::linear/MatMul\n",
      "797 __module.transformer_blocks.23.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1198 __module.transformer_blocks.23.ff.net.0.proj/aten::linear/MatMul\n",
      "1521 __module.transformer_blocks.23.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2063 __module.transformer_blocks.23.ff.net.2/aten::linear/MatMul\n",
      "2372 __module.transformer_blocks.23.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1201 __module.transformer_blocks.24.attn1.to_k/aten::linear/MatMul\n",
      "1524 __module.transformer_blocks.24.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1202 __module.transformer_blocks.24.attn1.to_q/aten::linear/MatMul\n",
      "1525 __module.transformer_blocks.24.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1203 __module.transformer_blocks.24.attn1.to_v/aten::linear/MatMul\n",
      "1526 __module.transformer_blocks.24.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3289 __module.transformer_blocks.24.attn1.to_out.0/aten::linear/MatMul\n",
      "3458 __module.transformer_blocks.24.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1527 __module.transformer_blocks.24.attn2.to_q/aten::linear/MatMul\n",
      "1800 __module.transformer_blocks.24.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 498 __module.transformer_blocks.24.attn2.to_out.0/aten::linear/MatMul\n",
      "798 __module.transformer_blocks.24.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1206 __module.transformer_blocks.24.ff.net.0.proj/aten::linear/MatMul\n",
      "1528 __module.transformer_blocks.24.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2072 __module.transformer_blocks.24.ff.net.2/aten::linear/MatMul\n",
      "2383 __module.transformer_blocks.24.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1209 __module.transformer_blocks.25.attn1.to_k/aten::linear/MatMul\n",
      "1531 __module.transformer_blocks.25.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1210 __module.transformer_blocks.25.attn1.to_q/aten::linear/MatMul\n",
      "1532 __module.transformer_blocks.25.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1211 __module.transformer_blocks.25.attn1.to_v/aten::linear/MatMul\n",
      "1533 __module.transformer_blocks.25.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3297 __module.transformer_blocks.25.attn1.to_out.0/aten::linear/MatMul\n",
      "3463 __module.transformer_blocks.25.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1534 __module.transformer_blocks.25.attn2.to_q/aten::linear/MatMul\n",
      "1808 __module.transformer_blocks.25.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 499 __module.transformer_blocks.25.attn2.to_out.0/aten::linear/MatMul\n",
      "799 __module.transformer_blocks.25.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1214 __module.transformer_blocks.25.ff.net.0.proj/aten::linear/MatMul\n",
      "1535 __module.transformer_blocks.25.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2081 __module.transformer_blocks.25.ff.net.2/aten::linear/MatMul\n",
      "2394 __module.transformer_blocks.25.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1217 __module.transformer_blocks.26.attn1.to_k/aten::linear/MatMul\n",
      "1538 __module.transformer_blocks.26.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1218 __module.transformer_blocks.26.attn1.to_q/aten::linear/MatMul\n",
      "1539 __module.transformer_blocks.26.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1219 __module.transformer_blocks.26.attn1.to_v/aten::linear/MatMul\n",
      "1540 __module.transformer_blocks.26.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3305 __module.transformer_blocks.26.attn1.to_out.0/aten::linear/MatMul\n",
      "3468 __module.transformer_blocks.26.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1541 __module.transformer_blocks.26.attn2.to_q/aten::linear/MatMul\n",
      "1816 __module.transformer_blocks.26.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 500 __module.transformer_blocks.26.attn2.to_out.0/aten::linear/MatMul\n",
      "800 __module.transformer_blocks.26.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1222 __module.transformer_blocks.26.ff.net.0.proj/aten::linear/MatMul\n",
      "1542 __module.transformer_blocks.26.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2090 __module.transformer_blocks.26.ff.net.2/aten::linear/MatMul\n",
      "2405 __module.transformer_blocks.26.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1225 __module.transformer_blocks.27.attn1.to_k/aten::linear/MatMul\n",
      "1545 __module.transformer_blocks.27.attn1.to_k/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1226 __module.transformer_blocks.27.attn1.to_q/aten::linear/MatMul\n",
      "1546 __module.transformer_blocks.27.attn1.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1227 __module.transformer_blocks.27.attn1.to_v/aten::linear/MatMul\n",
      "1547 __module.transformer_blocks.27.attn1.to_v/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3313 __module.transformer_blocks.27.attn1.to_out.0/aten::linear/MatMul\n",
      "3473 __module.transformer_blocks.27.attn1.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1548 __module.transformer_blocks.27.attn2.to_q/aten::linear/MatMul\n",
      "1824 __module.transformer_blocks.27.attn2.to_q/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 501 __module.transformer_blocks.27.attn2.to_out.0/aten::linear/MatMul\n",
      "801 __module.transformer_blocks.27.attn2.to_out.0/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1230 __module.transformer_blocks.27.ff.net.0.proj/aten::linear/MatMul\n",
      "1549 __module.transformer_blocks.27.ff.net.0.proj/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2099 __module.transformer_blocks.27.ff.net.2/aten::linear/MatMul\n",
      "2416 __module.transformer_blocks.27.ff.net.2/aten::linear/Add\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1031 __module.proj_out/aten::linear/MatMul\n",
      "1345 __module.proj_out/aten::linear/Add\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e7a00d8be74dceae60bfe9de6571a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "\n",
    "import nncf\n",
    "import logging\n",
    "from nncf.quantization.advanced_parameters import AdvancedSmoothQuantParameters\n",
    "\n",
    "from ltx_video_quantization_helper import get_operation_const_op, collect_ops_with_weights\n",
    "\n",
    "\n",
    "if not TRANSFORMER_INT8_PATH.exists():\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "\n",
    "    transformer_model = core.read_model(TRANSFORMER_OV_PATH)\n",
    "    transformer_ignored_scope = collect_ops_with_weights(transformer_model)\n",
    "    compressed_transformer_model = nncf.compress_weights(transformer_model)\n",
    "    quantized_transformer_model = nncf.quantize(\n",
    "        model=compressed_transformer_model,\n",
    "        calibration_dataset=nncf.Dataset(transformer_calibration_data),\n",
    "        subset_size=calibration_dataset_size,\n",
    "        model_type=nncf.ModelType.TRANSFORMER,\n",
    "        ignored_scope=nncf.IgnoredScope(names=transformer_ignored_scope),\n",
    "        advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alphas=AdvancedSmoothQuantParameters(matmul=-1))\n",
    "    )\n",
    "    ov.save_model(quantized_transformer_model, TRANSFORMER_INT8_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e1949a-a88f-451c-b148-d2f04b5d08c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08a515f61bb4c12a9f004fef246ff27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60666e945998478692c3d853efa1e4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "pipe = LTXPipeline.from_pretrained(\"Lightricks/LTX-Video\")\n",
    "\n",
    "compiled_transformer = core.compile_model(TRANSFORMER_INT8_PATH, device.value)\n",
    "compiled_vae = core.compile_model(VAE_DECODER_PATH, device.value)\n",
    "compiled_text_encoder = core.compile_model(TEXT_ENCODER_PATH, device.value)\n",
    "\n",
    "pipe.__dict__[\"_internal_dict\"][\"_execution_device\"] = pipe._execution_device  # this is to avoid some problem that can occur in the pipeline\n",
    "\n",
    "pipe.register_modules(\n",
    "    text_encoder=TextEncoderWrapper(compiled_text_encoder, text_encoder_dtype),\n",
    "    transformer=ConvTransformerWrapper(compiled_transformer, transformer_config),\n",
    "    vae=VAEWrapper(compiled_vae, vae_config, vae_latents_mean, vae_latents_std),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7c2ac-dfe6-4ce2-a914-d5b10813f19e",
   "metadata": {},
   "source": [
    "### Run Quantized OpenVINO model inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec2a48f8-5e08-4a79-b067-61d6f5dadc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/sd-xl/openvino_notebooks/venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `_execution_device` directly via 'LTXPipeline' object attribute is deprecated. Please access '_execution_device' over 'LTXPipeline's config object instead, e.g. 'scheduler.config._execution_device'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51c083175cd476799c32af8506d166f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "prompt = \"A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.\"\n",
    "negative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    width=704,\n",
    "    height=480,\n",
    "    num_frames=24,\n",
    "    num_inference_steps=40,\n",
    "    generator=generator,\n",
    "    guidance_scale=3,\n",
    ").frames[0]\n",
    "export_to_video(video, \"output_int8_ov.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "358c42ac-edd0-47af-8aee-41ce90370f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"output_int8_ov.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "display(Video(\"output_int8_ov.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc50fc1-e364-4f24-af8b-b38e939b72a5",
   "metadata": {},
   "source": [
    "### Compare Transformer File Size\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8466662-011b-403f-93a4-cf7897267f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 IR model size: 3668.57 MB\n",
      "INT8 model size: 1837.77 MB\n",
      "Model compression rate: 1.996\n"
     ]
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "fp16_ir_model_size = Path(TRANSFORMER_OV_PATH).with_suffix(\".bin\").stat().st_size / 1024 / 1024\n",
    "quantized_model_size = Path(TRANSFORMER_INT8_PATH).with_suffix(\".bin\").stat().st_size / 1024 / 1024\n",
    "print(f\"FP16 IR model size: {fp16_ir_model_size:.2f} MB\")\n",
    "print(f\"INT8 model size: {quantized_model_size:.2f} MB\")\n",
    "print(f\"Model compression rate: {fp16_ir_model_size / quantized_model_size:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc187762-b6ab-40f8-b061-3a0e8dec773e",
   "metadata": {},
   "source": [
    "### Compare inference time of the FP16 IR and quantized models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "To measure the inference performance of the `FP16` and `INT8` models, we use median inference time on calibration dataset.\n",
    "So we can approximately estimate the speed up of the dynamic quantized models.\n",
    "\n",
    "\n",
    "> **NOTE**: For the most accurate performance estimation, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications with static shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab7fbcf-e2b6-48df-bce3-29098038bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_inference_time(model_path, calibration_data):\n",
    "    model = core.compile_model(model_path, device.value)\n",
    "    inference_time = []\n",
    "    for batch in calibration_data:\n",
    "        start = time.perf_counter()\n",
    "        _ = model(batch)[0]\n",
    "        end = time.perf_counter()\n",
    "        delta = end - start\n",
    "        inference_time.append(delta)\n",
    "    return np.median(inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f328cda1-18d6-42c5-a350-7ad1cecf576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance speed up: 1.740\n"
     ]
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "fp16_latency = calculate_inference_time(TRANSFORMER_OV_PATH, transformer_calibration_data)\n",
    "int8_latency = calculate_inference_time(TRANSFORMER_INT8_PATH, transformer_calibration_data)\n",
    "print(f\"Performance speed up: {fp16_latency / int8_latency:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9ffa6-2beb-4680-b6ad-527d46160318",
   "metadata": {},
   "source": [
    "## Interactive inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Please select below whether you would like to use the quantized models to launch the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2c3542b-bba7-4b62-a0b8-18b66b015e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd56f2bf93f4ba58a1be27e94bb952c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Use quantized model')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "use_quantized_model = widgets.Checkbox(\n",
    "    description=\"Use quantized model\",\n",
    "    value=TRANSFORMER_INT8_PATH.exists(),\n",
    "    disabled=not TRANSFORMER_INT8_PATH.exists(),\n",
    ")\n",
    "\n",
    "use_quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c72ab-e220-483c-a7fa-2dd4a3cef173",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = LTXPipeline.from_pretrained(\"Lightricks/LTX-Video\")\n",
    "\n",
    "compiled_transformer = (\n",
    "    core.compile_model(TRANSFORMER_INT8_PATH, device.value) if use_quantized_model.value else core.compile_model(TRANSFORMER_OV_PATH, device.value)\n",
    ")\n",
    "compiled_vae = core.compile_model(VAE_DECODER_PATH, device.value)\n",
    "compiled_text_encoder = core.compile_model(TEXT_ENCODER_PATH, device.value)\n",
    "\n",
    "pipe.__dict__[\"_internal_dict\"][\"_execution_device\"] = pipe._execution_device  # this is to avoid some problem that can occur in the pipeline\n",
    "\n",
    "pipe.register_modules(\n",
    "    text_encoder=TextEncoderWrapper(compiled_text_encoder, text_encoder_dtype),\n",
    "    transformer=ConvTransformerWrapper(compiled_transformer, transformer_config),\n",
    "    vae=VAEWrapper(compiled_vae, vae_config, vae_latents_mean, vae_latents_std),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c9d37a4-88e2-4323-8a95-191c36f79276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate(prompt, negative_prompt, width, height, num_frames, num_inference_steps, seed, _=gr.Progress(track_tqdm=True)):\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    video = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        num_frames=num_frames,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=generator,\n",
    "    ).frames[0]\n",
    "    file_name = \"output.mp4\"\n",
    "    export_to_video(video, file_name, fps=24)\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85299212-79bb-42fc-9173-6db09bae41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/ltx-video/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "demo = make_demo(fn=generate)\n",
    "\n",
    "try:\n",
    "    demo.queue().launch(debug=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True, debug=True)\n",
    "# If you are launching remotely, specify server_name and server_port\n",
    "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
    "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00003.gif?raw=true",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Text-to-Video"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
